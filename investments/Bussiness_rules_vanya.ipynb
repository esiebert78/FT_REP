{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'holoviews' has no attribute 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a8ffbd3c7698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#import pandas_datareader as pdr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhvplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mholoviews\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hvplot\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_kind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mHoloViewsConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kind_mapping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_kind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0m_patch_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_kind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hvplot\\__init__.py\u001b[0m in \u001b[0;36m_patch_doc\u001b[1;34m(cls, kind)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_doc_and_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hvplot\\__init__.py\u001b[0m in \u001b[0;36m_get_doc_and_signature\u001b[1;34m(cls, kind, completions, docstring, generic, style)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     extra_kwargs = _hv.core.util.unique_iterator(\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mvalid_opts\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkind_opts\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axis_options\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'holoviews' has no attribute 'core'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "#import pandas_datareader as pdr\n",
    "import hvplot.pandas\n",
    "import seaborn as sns\n",
    "import holoviews as hv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import scipy.optimize as sco\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data files\n",
    "\n",
    "quandl_file = Path(\"Resources/Quandl_data.csv\")\n",
    "asset_file = Path(\"Resources/asset_prices.csv\")\n",
    "bitcoin_file = Path(\"Resources/bitcoin_data.csv\")\n",
    "\n",
    "alt_data_df = pd.read_csv(quandl_file, index_col = \"yr_mo\", infer_datetime_format = True, parse_dates = True)\n",
    "asset_prices_df = pd.read_csv(asset_file, index_col = \"Date\", infer_datetime_format = True, parse_dates = True)\n",
    "bitcoin_df = pd.read_csv(bitcoin_file, index_col = \"Date\" , infer_datetime_format = True, parse_dates = True)\n",
    "\n",
    "bitcoin_df.drop(columns = [\"Open*\", \"High\", \"Low\", \"Volume\", \"Market Cap\"], inplace=True)\n",
    "bitcoin_df.columns = [\"Bitcoin\"]\n",
    "bitcoin_df.sort_index()\n",
    "\n",
    "asset_prices_df.drop(columns = \"BLOK\", inplace= True)\n",
    "asset_prices_df = pd.merge(asset_prices_df, bitcoin_df, how = \"inner\", on= \"Date\" )\n",
    "asset_prices_df.sort_index(inplace=True)\n",
    "asset_prices_df[\"Bitcoin\"]= asset_prices_df[\"Bitcoin\"].str.replace(\",\", \"\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_prices_df.dropna(inplace=True)\n",
    "asset_prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_returns_monthly_df = asset_prices_df[asset_prices_df.index.day == 1].pct_change()\n",
    "asset_returns_monthly_df.dropna(inplace=True)\n",
    "asset_returns_monthly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_returns_df = asset_prices_df.pct_change().dropna()\n",
    "asset_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation\n",
    "assets_st_dev_19 = asset_returns_df[(asset_returns_df.index >= '2019-01-01') & (asset_returns_df.index < '2020-01-01') ].std()\n",
    "assets_st_dev_19 = pd.DataFrame(assets_st_dev_19, columns = ['STD_19'] )\n",
    "\n",
    "assets_st_dev_20 = asset_returns_df[asset_returns_df.index >= '2020-01-01'].std()\n",
    "assets_st_dev_20 = pd.DataFrame(assets_st_dev_20, columns = ['STD_20'] )\n",
    "\n",
    "#assets_st_dev_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annualized standard deviation\n",
    "annualized_st_dev = asset_returns_df.std() * np.sqrt(252)\n",
    "annualized_st_dev = pd.DataFrame(annualized_st_dev, columns = ['Annualized_STD'] )\n",
    "#annualized_st_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sharpe ratios\n",
    "\n",
    "sharpe_ratios_df = (asset_returns_df.mean() * 252) / (asset_returns_df.std() * np.sqrt(252))\n",
    "sharpe_ratios_df = pd.DataFrame( sharpe_ratios_df, columns = ['Sharpe_Ratios'] )\n",
    "#sharpe_ratios_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average return over the whole period\n",
    "mean_daily_return_df_19 = pd.DataFrame(asset_returns_df[(asset_returns_df.index>='2019-01-01') & \n",
    "                                                        (asset_returns_df.index<'2020-01-01')].mean(),\n",
    "                                                         columns = ['Avg_daily_return_2019'])\n",
    "\n",
    "mean_daily_return_df_20 = pd.DataFrame(asset_returns_df[(asset_returns_df.index>='2020-01-01')].mean(),\n",
    "                                                         columns = ['Avg_daily_return_2020'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BETA FUNCTION\n",
    "\n",
    "def calculate_beta(data, market_variable = \"SPY\"):\n",
    "    \n",
    "    beta_df = pd.DataFrame(index=[1])\n",
    "    \n",
    "    for i in data.columns:\n",
    "        \n",
    "        variance = data[i].var()\n",
    "        covariance = data[i].cov(data[market_variable])\n",
    "        beta = covariance / variance\n",
    "\n",
    "        beta_df[i]= beta\n",
    "    \n",
    "    return beta_df.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta represents the volatility compared to the market, beta more then 1 means the asset is more riskier\n",
    "\n",
    "beta_df_19 = calculate_beta(asset_returns_df[(asset_returns_df.index>='2019-01-01') & \n",
    "                                            (asset_returns_df.index<'2020-01-01')], \"SPY\")\n",
    "beta_df_19.columns = ['Beta_19']\n",
    "\n",
    "beta_df_20 = calculate_beta(asset_returns_df[(asset_returns_df.index>='2020-01-01')], \"SPY\")\n",
    "beta_df_20.columns = ['Beta_30']\n",
    "#beta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_returns_monthly_df[asset_returns_monthly_df.index>'2019-01-01'].hvplot(title = \"Asset Returns 2019-2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(asset_returns_monthly_df, alt_data_df, how=\"inner\",left_index = True, right_index=True)\n",
    "all_data.dropna(inplace = True)\n",
    "all_data.drop(columns = [\"SP500_pe_shiller\", \"SP500_eyield\", \"15y_mort_int\", 'SP500_pe','Unemployment_LT','GDP_R'], inplace=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = all_data.corr()\n",
    "sns.heatmap(correlation,vmin=-1, vmax=1, linewidths = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsi_timeseries(prices, n=14):\n",
    "    # RSI = 100 - (100 / (1 + RS))\n",
    "    # where RS = (Wilder-smoothed n-period average of gains / Wilder-smoothed n-period average of -losses)\n",
    "    # Note that losses above should be positive values\n",
    "    # Wilder-smoothing = ((previous smoothed avg * (n-1)) + current value to average) / n\n",
    "    # For the very first \"previous smoothed avg\" (aka the seed value), we start with a straight average.\n",
    "    # Therefore, our first RSI value will be for the n+2nd period:\n",
    "    #     0: first delta is nan\n",
    "    #     1:\n",
    "    #     ...\n",
    "    #     n: lookback period for first Wilder smoothing seed value\n",
    "    #     n+1: first RSI\n",
    "\n",
    "    # First, calculate the gain or loss from one price to the next. The first value is nan so replace with 0.\n",
    "    deltas = (prices-prices.shift(1)).fillna(0)\n",
    "\n",
    "    # Calculate the straight average seed values.\n",
    "    # The first delta is always zero, so we will use a slice of the first n deltas starting at 1,\n",
    "    # and filter only deltas > 0 to get gains and deltas < 0 to get losses\n",
    "    avg_of_gains = deltas[1:n+1][deltas > 0].sum() / n\n",
    "    avg_of_losses = -deltas[1:n+1][deltas < 0].sum() / n\n",
    "\n",
    "    # Set up pd.Series container for RSI values\n",
    "    rsi_series = pd.Series(0.0, deltas.index)\n",
    "\n",
    "    # Now calculate RSI using the Wilder smoothing method, starting with n+1 delta.\n",
    "    up = lambda x: x if x > 0 else 0\n",
    "    down = lambda x: -x if x < 0 else 0\n",
    "    i = n+1\n",
    "    \n",
    "    for d in deltas[n+1:]:\n",
    "        avg_of_gains = ((avg_of_gains * (n-1)) + up(d)) / n\n",
    "        avg_of_losses = ((avg_of_losses * (n-1)) + down(d)) / n\n",
    "        \n",
    "        if avg_of_losses != 0:\n",
    "            rs = avg_of_gains / avg_of_losses\n",
    "            rsi_series[i] = 100 - (100 / (1 + rs))\n",
    "        else:\n",
    "            rsi_series[i] = 100\n",
    "        i += 1\n",
    "\n",
    "    return rsi_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsi_df = pd.DataFrame()\n",
    "\n",
    "for i in asset_prices_df.columns :\n",
    "    rsi_df[i] = get_rsi_timeseries(asset_prices_df[i])\n",
    "    \n",
    "\n",
    "rsi_df[rsi_df.index >'2020-03-01'].hvplot()                                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KPI table\n",
    "KPI_df = pd.merge(mean_daily_return_df_19, mean_daily_return_df_20, how =\"inner\", left_index = True, right_index=True)\n",
    "KPI_df = pd.merge(KPI_df, assets_st_dev_19,how =\"inner\", left_index = True, right_index=True)\n",
    "KPI_df = pd.merge(KPI_df, assets_st_dev_20,how =\"inner\", left_index = True, right_index=True)\n",
    "#KPI_df = pd.merge(KPI_df , annualized_st_dev, how =\"inner\", left_index = True, right_index=True)\n",
    "KPI_df = pd.merge(KPI_df, sharpe_ratios_df ,  how =\"inner\", left_index = True, right_index=True)\n",
    "KPI_df = pd.merge(KPI_df, beta_df_19 ,  how =\"inner\", left_index = True, right_index=True)\n",
    "KPI_df = pd.merge(KPI_df, beta_df_20 ,  how =\"inner\", left_index = True, right_index=True)\n",
    "KPI_df[\"RSI\"] = rsi_df.iloc[-1]\n",
    "#volume\n",
    "\n",
    "\n",
    "KPI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logarythmic returns\n",
    "log_ret = np.log(asset_prices_df[asset_prices_df.index > '2019-01-01']/asset_prices_df[asset_prices_df.index > '2019-01-01'].shift(1))\n",
    "log_ret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ret.hvplot.hist(bins=100, subplots=True, width=500, group_label='Ticker', grid=True).cols(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#efficient portfolio frontier plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ports = 10000\n",
    "\n",
    "all_weights = np.zeros((num_ports,len(asset_returns_df.columns)))\n",
    "ret_arr = np.zeros(num_ports)\n",
    "vol_arr = np.zeros(num_ports)\n",
    "sharpe_arr = np.zeros(num_ports)\n",
    "\n",
    "for ind in range(num_ports):\n",
    "\n",
    "    # Create Random Weights\n",
    "    weights = np.array(np.random.random(12))\n",
    "\n",
    "    # Rebalance Weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Save Weights\n",
    "    all_weights[ind,:] = weights\n",
    "\n",
    "    # Expected Return\n",
    "    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n",
    "\n",
    "    # Expected Variance\n",
    "    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n",
    "\n",
    "    # Sharpe Ratio\n",
    "    sharpe_arr[ind] = ret_arr[ind]/vol_arr[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for th epoint with the highest sharpe ratio\n",
    "\n",
    "max_sr_ret = ret_arr[sharpe_arr.argmax()]\n",
    "max_sr_vol = vol_arr[sharpe_arr.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = hv.Scatter((vol_arr, ret_arr, sharpe_arr), 'Volatility', ['Return', 'Sharpe Ratio'])\n",
    "max_sharpe = hv.Scatter([(max_sr_vol,max_sr_ret)])\n",
    "\n",
    "scatter.opts(color='Sharpe Ratio', cmap='plasma', width=600, height=400, colorbar=True, padding=0.1) *\\\n",
    "max_sharpe.opts(color='red', line_color='black', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ret_vol_sr(weights):\n",
    "    \"\"\"\n",
    "    Takes in weights, returns array or return,volatility, sharpe ratio\n",
    "    \"\"\"\n",
    "    weights = np.array(weights)\n",
    "    ret = np.sum(log_ret.mean() * weights) * 252\n",
    "    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n",
    "    sr = ret/vol\n",
    "    return np.array([ret,vol,sr])\n",
    "\n",
    "\n",
    "def neg_sharpe(weights):\n",
    "    return  get_ret_vol_sr(weights)[2] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "#help.minimize\n",
    "\n",
    "# Contraints\n",
    "def check_sum(weights):\n",
    "    '''\n",
    "    Returns 0 if sum of weights is 1.0\n",
    "    '''\n",
    "    return np.sum(weights) - 1\n",
    "\n",
    "# By convention of minimize function it should be a function that returns zero for conditions\n",
    "cons = ({'type':'eq','fun': check_sum})\n",
    "\n",
    "# 0-1 bounds for each weight\n",
    "bounds = ((0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1))\n",
    "\n",
    "# Initial Guess (equal distribution)\n",
    "init_guess = [1,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Sequential Least SQuares Programming (SLSQP).\n",
    "opt_results = minimize(neg_sharpe,init_guess,method='SLSQP',bounds=bounds,constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linspace number of points to calculate x on\n",
    "frontier_y = np.linspace(0,0.25,20) # Change 100 to a lower number for slower computers!]\n",
    "\n",
    "def minimize_volatility(weights):\n",
    "    return  get_ret_vol_sr(weights)[1] \n",
    "frontier_volatility = []\n",
    "\n",
    "for possible_return in frontier_y:\n",
    "    # function for return\n",
    "    \n",
    "    cons = ({'type':'eq','fun': check_sum},\n",
    "            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n",
    "    \n",
    "    result = minimize(minimize_volatility,init_guess,method='SLSQP',bounds=bounds,constraints=cons)\n",
    "    \n",
    "    frontier_volatility.append(result['fun'])\n",
    "    \n",
    "scatter * hv.Curve((frontier_volatility, frontier_y)).opts(color='green', line_dash='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ret_vol_sr(opt_results.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_arr.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional portfolio\n",
    "\n",
    "\n",
    "KPI_df['Optimal_Portfolio'] = all_weights[sharpe_arr.argmax(),:]\n",
    "\n",
    "KPI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal portfolio test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def portfolio_annualised_performance(weights, mean_returns, cov_matrix):\n",
    "    returns = np.sum(mean_returns*weights ) *252\n",
    "    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "    return std, returns\n",
    "  \n",
    "def random_portfolios(num_portfolios, mean_returns, cov_matrix, risk_free_rate):\n",
    "    results = np.zeros((3,num_portfolios))\n",
    "    weights_record = []\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(12)\n",
    "        weights /= np.sum(weights)\n",
    "        weights_record.append(weights)\n",
    "        portfolio_std_dev, portfolio_return = portfolio_annualised_performance(weights, mean_returns, cov_matrix)\n",
    "        results[0,i] = portfolio_std_dev\n",
    "        results[1,i] = portfolio_return\n",
    "        results[2,i] = (portfolio_return - risk_free_rate) / portfolio_std_dev\n",
    "    return results, weights_record\n",
    "\n",
    "def display_simulated_ef_with_random(mean_returns, cov_matrix, num_portfolios, risk_free_rate):\n",
    "    results, weights = random_portfolios(num_portfolios,mean_returns, cov_matrix, risk_free_rate)\n",
    "    \n",
    "    max_sharpe_idx = np.argmax(results[2])\n",
    "    sdp, rp = results[0,max_sharpe_idx], results[1,max_sharpe_idx]\n",
    "    max_sharpe_allocation = pd.DataFrame(weights[max_sharpe_idx],index=asset_returns_df.columns,columns=['allocation'])\n",
    "    max_sharpe_allocation.allocation = [round(i*100,2)for i in max_sharpe_allocation.allocation]\n",
    "    max_sharpe_allocation = max_sharpe_allocation.T\n",
    "    \n",
    "    min_vol_idx = np.argmin(results[0])\n",
    "    sdp_min, rp_min = results[0,min_vol_idx], results[1,min_vol_idx]\n",
    "    min_vol_allocation = pd.DataFrame(weights[min_vol_idx],index=asset_returns_df.columns,columns=['allocation'])\n",
    "    min_vol_allocation.allocation = [round(i*100,2)for i in min_vol_allocation.allocation]\n",
    "    min_vol_allocation = min_vol_allocation.T\n",
    "    \n",
    "    print (\"-\"*80)\n",
    "    print (\"Maximum Sharpe Ratio Portfolio Allocation\\n\")\n",
    "    print (\"Annualised Return:\", round(rp,2))\n",
    "    print (\"Annualised Volatility:\", round(sdp,2))\n",
    "    print (\"\\n\")\n",
    "    print (max_sharpe_allocation)\n",
    "    print (\"-\"*80)\n",
    "    print (\"Minimum Volatility Portfolio Allocation\\n\")\n",
    "    print (\"Annualised Return:\", round(rp_min,2))\n",
    "    print (\"Annualised Volatility:\", round(sdp_min,2))\n",
    "    print (\"\\n\")\n",
    "    print (min_vol_allocation)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(results[0,:],results[1,:],c=results[2,:],cmap='YlGnBu', marker='o', s=10, alpha=0.3)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(sdp,rp,marker='*',color='r',s=500, label='Maximum Sharpe ratio')\n",
    "    plt.scatter(sdp_min,rp_min,marker='*',color='g',s=500, label='Minimum volatility')\n",
    "    plt.title('Simulated Portfolio Optimization based on Efficient Frontier')\n",
    "    plt.xlabel('annualised volatility')\n",
    "    plt.ylabel('annualised returns')\n",
    "    plt.legend(labelspacing=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = asset_returns_df\n",
    "mean_returns = returns.mean()\n",
    "cov_matrix = returns.cov()\n",
    "num_portfolios = 25000\n",
    "risk_free_rate = asset_returns_df['STIP'].iloc[-1]\n",
    "\n",
    "\n",
    "display_simulated_ef_with_random(mean_returns, cov_matrix, num_portfolios, risk_free_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
